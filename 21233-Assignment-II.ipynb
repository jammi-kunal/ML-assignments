{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions-I from MDSC-301(P)\n",
    "\n",
    "----------------------------------------------------------------\n",
    "Author: Dr. Sampath Lonka \n",
    "\n",
    "Date: August 20, 2022\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Which Linear Regression training algorithm can you use if you have\n",
    "a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1. Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Suppose the features in your training set have very different scales.\n",
    "Which algorithms might suffer from this, and how? What can you\n",
    "do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2. Algorithms which might suffer from this are the distance based algorithms\n",
    "- Gradient Descent\n",
    "- KNN Classifier\n",
    "- K-means\n",
    "- SVM \n",
    "\n",
    "Let us consider an example,\n",
    "Suppose we have two features, <b>width and height</b>, <u>width is in centimeters, <b>'cm'</b></u>, and <u>height is in feet, <b>'ft'</b></u>, thus, if we fit our KNN or Regression model to it, the width values being very small when compared to height values will be lesser significant and the model will tend to ignore it and classify solely based on the height values.\n",
    "If the scales are different then many features might appear to be very significant due to their large values and many other significant but incorrectly scaled features might have very less weight in the model and eventually be ignored and lead to an incorrect prediction.\n",
    "\n",
    "We can solve it by using normalization or standardization.\n",
    "- Normalization - the values are shifted and rescaled such that they appera between 0 and 1. \n",
    "$$X^`=\\frac{(X - X_{min})}{(X_{max} - X_{min})}$$\n",
    "- Standardization - rescaling the values to their Z-score, or subtracting from mean and dividing by their standard deviation.\n",
    "$$X_{new}=\\frac{(X - X_{meam})}{Std}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.  Suppose you use Batch Gradient Descent and you plot the validation\n",
    "error at every epoch. If you notice that the validation error\n",
    "consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3. Overfitting\n",
    "- Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Is it a good idea to stop Mini-batch Gradient Descent immediately\n",
    "when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A4. No, because it works on mini-batches or subsets of the original data and hence the validation error might improve and decrease as we further proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Suppose you are using Polynomial Regression. You plot the learning\n",
    "curves and you notice that there is a large gap between the training\n",
    "error and the validation error. What is happening? What are three\n",
    "ways to solve this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A5. It means that we are using a high degree polynomial which is leading to overfitting of the training data.\n",
    "\n",
    "Three ways to solve this:\n",
    "1. Use lower degree polynomials\n",
    "2. Regularization\n",
    "3. k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Suppose you are using Ridge Regression and you notice that the\n",
    "training error and the validation error are almost equal and fairly\n",
    "high. Would you say that the model suffers from high bias or high\n",
    "variance? Should you increase the regularization hyperparameter $\\alpha$\n",
    "or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A6. It suffers from high variance. We must increase the hyperparameter $\\alpha$ in order to reduce the complexity of the model and make it more generalized and lower the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Why would you want to use:\n",
    "   - Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?\n",
    "   - Lasso instead of Ridge Regression?\n",
    "   - Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A7.\n",
    "- Ridge Regression (L2 regularization) - to avoid overfitting, to keep the weights as small as possible. Includes all the features in the model\n",
    "- Lasso Regression instead of ridge regression - Lasso regression can zero out the coefficients, which ends up in no contribution from that particular feature, hence it does feature selection and parameter shrinkage automatically. It gives us a sparse model whereas ridge gives us a dense model.\n",
    "- Lasso regression might not perform very well in the case of highly correlated data, hence a combination of the two, or a tradeoff, Elastic Net regression was introduced which performs fairly well with highly correlated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8.  Can you name four of the main challenges in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A8. Four main challenges of Machine Learning are: \n",
    "1. Overfitting and Underfitting (Bias / Variance Tradeoff)\n",
    "2. Need of large and quality training data and good computational resources.\n",
    "3. Faster computational runtimes\n",
    "4. Monitoring and Maintainance of the models with time.\n",
    "5. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. If your model performs great on the training data but generalizes\n",
    "poorly to new instances, what is happening? Can you name three\n",
    "possible solutions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A9. The model is overfitting on the training data and not able to generalize well to the unseen data.\n",
    "Possible solutions are:\n",
    "1. Adding a Regularization term - helps in reducing the complexity of the model\n",
    "2. Cross Validation - Cross Validation might help us to detect overfitting and early stopping methods might help to stop the training process when overfit detected.\n",
    "3. More Training data - We can also maybe train with more data, so that the model is able to understand better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is a test set, and why would you want to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A10. The test set is used for testing the performance of the model. It tells us if the model is able to generalize well on the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11.  What is the purpose of a validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A11. The validation set is used to check the performance of the model after every epoch, so that if the model starts to overfit after a certain epoch then we can stop it early and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. What are different loss functions? Exaplain their importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A12. Different loss functions:\n",
    "1. MSE and RMSE - It is the average of the squared distance between the actual and predicted values. \n",
    "    - Due to squaring the predictions are penalized heavily.\n",
    "    - Formula for MSE : ${\\sum_{i=1}^{n}\\frac{y_{i} - \\hat{y}_{i}}{n}}$\n",
    "    - Formula for RMSE : $\\sqrt{{\\sum_{i=1}^{n}\\frac{y_{i} - \\hat{y}_{i}}{n}}}$\n",
    "2. MAE - Mean absolute loss. \n",
    "    - Measures the magnitude of difference between the actual and predicted values\n",
    "3. Hinge Loss - \n",
    "    - Not differentiable loss. \n",
    "    - Also known as SVM loss. \n",
    "    - Convex function. \n",
    "    - Used for classification\n",
    "4. Cross Entropy Loss - \n",
    "    - Used for classification.\n",
    "    - Known as Negative Log Likelihood Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Explain the following-\n",
    "   - Gradient descent\n",
    "   - Mini-batch gradient descent\n",
    "   - Batch gradient, and\n",
    "   - Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A13.\n",
    "1. Gradient Descent - \n",
    "    - It is an iterative algorithm used to find the maximum/minimum of a differentiable and convex function.\n",
    "    - It moves towards the minimum of a convex function. \n",
    "    - The next step is calculated by taking the current value and subtracting it with the product of learning rate $(\\lambda)$ and gradient at the current position.\n",
    "    - This way it moves towards the minimum.\n",
    "\n",
    "2. Batch Gradient Descent - \n",
    "    - In this method, all the training data is considered to take a single step.\n",
    "    - The average of all the gradients of all the training examples is used to update the parameters\n",
    "    \n",
    "3. Stochastic Gradient Descent - \n",
    "    - In Batch GD we consider all the training examples but if our dataset is very large then it will be difficult to calculate the gradients of all training examples at once.\n",
    "    - Hence here, we consider just one training example at a single time step.\n",
    "    - SGD works well with large datasets and converges faster and also updates the paramaters frequently.\n",
    "    - The drawback of this is that it cannot be vectorized and hence is slow.\n",
    "    \n",
    "4. Mini Batch Gradient Descent - \n",
    "    - Here, we neither use the entire training set nor single examples but we divide the data into mini batches. \n",
    "    - This gives us the advantages of both the Batch and SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. What is learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A14. Learning rate scales the gradient and hence controls the step size. It has a strong influence on the performance and convergence to the minimum.\n",
    "- The smaller the learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point\n",
    "- If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. Define the following terms. Explain their importance in the data analysis.\n",
    "    - $R^2$\n",
    "    - Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A15.\n",
    "\n",
    "$R^2$ - This tells us the degree of variance in the target variable, which is explained by our model.<br>\n",
    "- The Total sum of squares is given by: $TSS = {\\sum({y_{i} - \\bar{y}}})^2$\n",
    "- The Residual sum of squares is given by: $RSS = {\\sum({y_{i} - \\hat{y}}})^2$\n",
    "- Thus, the $R^2$ is given by: $\\frac{(TSS - RSS)}{TSS}$ \n",
    "\n",
    "$Adj. R^2$ - The R^2 value never decreases whenever we add a new feature. Even if the feature dosent contribute anything to the predictions, even then the R^2 value increases, hence it is flawed. So, we introduce a new stastistic, i.e, the Adjusted R-squared which takes into account the number of independent variables used for predicting the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q16. Explain One-Hot Encoding and Label Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A16.\n",
    "1. One-Hot Encoding : This is a method to convert a categorical feature  to a numerical feature. If the feature is nominal, i.e, arithmetic operations cannot be applied, such as , gender, address, etc. So, we apply one-hot encoding here. What it does is that it takes all the unique values in the feature and create a seperate feature for that unique value where all the indices with that value in the original feature are marked as 1 and rest as 0.\n",
    "\n",
    "2. Label Encoding : This is also a method of converting categorical features into numerical. It assigns a unique number to each class of the data and all the data is represented into a numeric vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17. What are the assumption on Naive Bayes algorithm in classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A17. Assumptions on the Naive Bayes algorithm: \n",
    "- All the features are independent\n",
    "- Each feature is given the same importance.\n",
    "- Or, all the features are independent and identical and make an equal contribution to the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q18. What is the difference between classification and regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A18. The fundamental difference between classification and regression is that, \n",
    "- classification is used to predict discrete class labels where as,\n",
    "- regression is used to predict continuous quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q19. How to ensure that the model is not overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A19.\n",
    "1. Split the data into test, train and valid sets.\n",
    "2. Cross-validation\n",
    "3. Larger dataset or more data, if not, then data augumentation\n",
    "4. Feature selection, to reduce the complexity of the model\n",
    "5. Regularization\n",
    "7. Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q20. List the main advantage of Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A20.\n",
    "1. Simple and easy to implement\n",
    "2. Can work with both continuous and discrete data\n",
    "3. Works with limited data\n",
    "4. It is very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q21. What you shoud do when your model is suffereing from:\n",
    "    - Low bias and high variance?\n",
    "    - High bias and low variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A21.\n",
    "- High Bias and Low Variance - \n",
    "    - Regularization\n",
    "    - Add more training data\n",
    "- Low Bias and High Variance - \n",
    "    - increasing the complexity of the model to better fit the data\n",
    "    - decreasing the regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q22. What is the 'Naive' in the Naive Bayes Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A22. The assumption that all the features are Independent and Identically distributed and contributed equally to the predicted value is 'naive'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q23. What is bias-variance tradeoff in Machine Learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A23. Ideally we want a machine learning model which takes into account all the patterns as well as the outliers in the training data and generalize them to the test data in order to achieve very small error and very high accuracy. High variance models are complex and represent all the features of the training set very well leading to minimal error on the training set but fail to generalize to the unseen data. In contrast, high bias models represent extremely simple mappings and can generalize some features to the unseen data, but the simplicity of these models leads to underfitting on the training set and generates predictions with lower variance (high bias) when applied to data outside of the training set. The ideal amount of bias and variance that a particular machine learning model should have depends on the minimization of the error (which includes bias error, variance error and noise). This is the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q24. Explain different trade-offs in Machine Learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A24. \n",
    "- Bias-Variance tradeoff : The problem of balancing the bias and variance of the model.\n",
    "- Precision-Recall tradeoff : Some models may have high precision but low recall and others with low precision and high recall. Good models must have a high precision and high recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q25. What is cross-validation and how it is useful in traing ML models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A25. It is a technique used to measure the performance of ML models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, we make a fixed number of folds (or partitions) of the data, run the model on each fold, and then average the overall error estimate.\n",
    "\n",
    "Cross-Validation is a very useful technique to assess the effectiveness of a machine learning model, particularly in cases where we need to prevent overfitting. It can also be used to tune the hyperparameters of the model with respect to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
